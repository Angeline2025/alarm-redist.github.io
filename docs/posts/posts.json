[
  {
    "path": "posts/2023-06-14-census-bias-and-noise-wp/",
    "title": "Working Paper: Evaluating Bias and Noise Induced by the U.S. Census Bureau's Privacy Protection Methods",
    "description": "Our new working paper uses the new Noisy Measurement File release to understand bias and noise caused by swapping (1990-2010) and the TopDown algorithm (2020).",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-06-14",
    "categories": [],
    "contents": "\nWe are excited to announce a new working paper Evaluating Bias and Noise Induced by the U.S. Census Bureau’s Privacy Protection Methods. This paper is the first independent evaluation of effects of the Census Bureau’s privacy protection on noise and bias in released counts. We leverage the recent release of the Noisy Measurements file (NMF) to evaluate both swapping and the newer TopDown algorithm.\nWe find that:\nthe NMF is too noisy to use alone, but the post-processing step of the TopDown algorithm reduces error substantially, making the post-processed data as accurate as swapping.\nerrors from privacy protection methods are generally smaller than other sources of census errors, they can be substantial for census geographies with small populations.\nthe average bias is fairly low across groups, but there is more uncertainty and noise for Hispanic and multiracial people. Bias and RMSE estimates nationally for 5 geographic levels are displayed by race group below.\n\n\n\nThe full abstract is below:\n\nThe United States Census Bureau faces a difficult trade-off between the accuracy of Census statistics and the protection of individual information. We conduct the first independent evaluation of bias and noise induced by the Bureau’s two main disclosure avoidance systems: the TopDown algorithm employed for the 2020 nsus and the swapping algorithm implemented for the 1990, 2000, and 2010 Censuses. Our evaluation leverages the recent release of the Noisy Measure File (NMF) as well as the availability of two independent runs of the TopDown algorithm applied to the 10 decennial Census. We find that the NMF contains too much noise to be directly useful alone, especially for Hispanic and multiracial populations. TopDown’s post-processing dramatically reduces the NMF noise and produces similarly accurate data to swapping in terms of bias and noise. These patterns hold across census geographies with varying population sizes and racial diversity. While the estimated errors for both TopDown and swapping are generally no larger than other sources of Census error, they can be relatively substantial for geographies with small total populations.\n\n\n\n\n",
    "preview": "posts/2023-06-14-census-bias-and-noise-wp/bias_rmse_by_race.png",
    "last_modified": "2023-08-01T12:47:25-04:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 2700
  },
  {
    "path": "posts/2023-06-13-widespread-gerrymandering-pnas/",
    "title": "Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition Published in PNAS",
    "description": "Our paper which details gerrymandering and partisan fairness in the 2022 redistricting maps is now published in PNAS.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-06-13",
    "categories": [],
    "contents": "\nToday, our paper Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition was published in PNAS.\nWe evaluate partisan gerrymandering nationwide for new 2020 districts using simulated maps that account for geography and state-specific rules. We find evidence that gerrymandering is widespread across states, resulting in disadvantages for the Democratic party and less competitive districts. Read the abstract below:\n\nRedistricting plans in legislatures determine how voters’ preferences are translated into representative’s seats. Political parties may manipulate the redistricting process to gain additional seats and insulate incumbents from electoral competition, a process known as gerrymandering. But detecting gerrymandering is difficult without a representative set of alternative plans that comply with the same geographic and legal constraints. Harnessing recent algorithmic advances in sampling, we study such a collection of alternative redistricting plans that can serve as a non-partisan baseline. This methodological approach can distinguish electoral bias due to partisan effects from electoral bias due to other factors. We find that Democrats are structurally and geographically disadvantaged in House elections by 8 seats, while partisan gerrymandering disadvantages them by 2 seats.\n\nIf you’re interested in further details on this research project, take a look at the Supplementary Information or our replication data.\nThis research would not be possible without the 50-State Redistricting Simulations. A special thank you to George Garcia, Kevin Wang, and Melissa Wu.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-01T12:47:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-03-census-letter/",
    "title": "Letter: Researchers need better access to US Census data published in Science",
    "description": "Our letter providing recommendations to the Census Bureau about the Noisy Measurements File (NMF) now published in Science.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-06-03",
    "categories": [],
    "contents": "\nFor the 2020 decennial census, the Census Bureau adopted a new Disclosure Avoidance System (DAS) based on differential privacy.\nThe DAS was designed to protect the confidentiality of responses by injecting statistical noise into a confidential individual census dataset.\nA key output of this system is the Noisy Measurement File (NMF), which is produced by adding random noise to tabulated statistics.\nThe resulting Noisy Measurement File (NMF) is an invaluable resource for Census data users to understand the error introduced by the DAS and perform statistically valid analyses that properly account for DAS-introduced error.\nThe Bureau did not initially release the NMF, but released a demonstration version in April 2023 after several public requests and subsequent litigation. The Bureau plans to release the NMF for the P.L.94-171 redistricting data and more detailed census data (the DHC file) later this year.\nWe commend the Bureau’s decision to provide the NMF, which will help advance social science research, improve policy decisions, and further strengthen the DAS itself.\nTo maximize the benefits of the released NMF, however, we believe that the Bureau must substantially improve the way in which the NMF is formatted and released.\nIn a letter recently published in Science, we explain several obstacles researchers may face when accessing, processing, and using the demonstration data for statistical analyses. We include several recommendations for the Bureau for future NMF releases.\nOur longer version of this letter describes how researchers can transform the NMF into a usable format and includes more detailed recommendations.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-01T12:47:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-14-widespread-gerrymandering-in-pnas/",
    "title": "Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition Forthcoming in PNAS",
    "description": "Our paper describing partisan gerrymandering and competition in the 2022 US congressional districts is now forthcoming in PNAS.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-04-14",
    "categories": [],
    "contents": "\n\n\n\nWe are delighted to announce that “Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition” has been accepted for publication in PNAS, the Proceedings of the National Academy of Sciences of the United States of America. This article evaluates the partisan effects of redistricting in 2020 using simulations, allowing us to account for both compliance with legal requirements and the effects of political geography. While many individual state plans are biased in favor of one party or the other, the US House plan for 2022 is relatively unbiased. However, many districts are quite a bit less competitive than would be typically expected.\nA pre-print of the article is available on arXiv.\nWe are grateful to the editors and reviewers for their helpful feedback in improving this paper.\n\n\n\n",
    "preview": "posts/2023-04-14-widespread-gerrymandering-in-pnas/manipulation.png",
    "last_modified": "2023-04-14T20:20:46-04:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4350
  },
  {
    "path": "posts/2023-03-18-redist-41/",
    "title": "redist 4.1",
    "description": "A medium-sized release with more flexible plotting, better diagnostics, and\nspeed improvements.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      }
    ],
    "date": "2023-03-19",
    "categories": [],
    "contents": "\nIt’s been a while since redist 4.0 was released and things have been fairly stable. Most of the changes in this release are behind-the-scenes improvements that shouldn’t break your workflow, but should improve your experience using the package.\nTo install version 4.1, get the new version from CRAN:\n\n\ninstall.packages('redist')\n\n\nNew Features\nExtends the ordered box/jitter plots to custom ordered geometries in redist.plot.distr_qtys()\nBetter diagnostic outputs for summary.redist_plans()\nImproved confidence intervals with redist_ci()\nC++ improvements for sampling more quickly\nBetter sampling efficiency in SMC’s final stage\nQuicker random walks for SMC and merge-split.\nFaster random number generation. (It’s small, but it adds up!)\n\nPlotting Flexibility with redist.plot.distr_qtys()\nBox-and-whiskers plots are great and useful in many situations. In redistricting, we’ve often used ordered boxplots. These order the x-axis by the quantity on the x-axis. Sometimes, a boxplot throws away information that you might care about, though.\n>Is the distribution multi-modal? Where are the 2.5th and 97.5th percentiles for a confidence interval?\nNow, you can take those questions into your own hands with adjustments to arguments in redist.plot.distr_qtys()!\nLet’s build this out a bit. First, we’ll use some data from the 50-State Redistricting Simulations via the alarmdata package.\nWe can get the redist_map and corresponding 5,000 sampled plus the enacted plans in a redist_plans object for Michigan.\n\n\nlibrary(dplyr)\nlibrary(redist)\nlibrary(alarmdata)\nmap <- alarm_50state_map('MI')\nplans <- alarm_50state_plans('MI')\n\n\nplans here has a column e_dvs that gives the expected Democratic vote share for each district in each plan.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs)\n\n\n\nWe’ve always been able to clean or augment this plot up using regular ggplot2 things:\n\n\nlibrary(ggplot2)\n\nredist.plot.distr_qtys(plans, qty = e_dvs) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nYou can draw clear conclusions here, like that districts 2, 3, and 4 are abnormally packed with Republicans compared to what we might normally see when drawing districts that follow the state’s redistricting rules. We might not be able to get all of the information out of this plot that we want, beyond which districts are clear outliers.\nNow, if we made it a box plot instead of a jitter plot, as the points can be overwhelming, we can still draw the same major conclusions, and we now have a more formal idea of outliers that don’t just sit above or below the data. Things like district 6 can be still be a bit unclear. Is district 6 in the outlier range or are the points just big on this small plot?\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = 'boxplot') + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nA first thing we might consider doing is to use a violin plot instead of a boxplot, as that doesn’t summarize distributional information in the same way as a box plot. This is now really easy, just pass ggplot2::geom_violin as an argument to geom.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = ggplot2::geom_violin) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nThe defaults here don’t always play the best though, so we might want to also change the reference geometry.\n\n\nr_geom <- function(...) \n    ggplot2::geom_segment(\n        ggplot2::aes(x = as.integer(.distr_no) - 0.5,\n                     xend = as.integer(.distr_no) + 0.5,\n                     yend = e_dvs,\n                     color = .data$draw),\n        ...\n    )\n\n\nThis immediately gets a bit more complicated. For this to work, we need to know a few things:\nThe function has to take ... as an argument.\nInternally, the variable we are plotting on the x is going to be called .distr_no.\nThe reference geometry will inherit x = .distr_no by default and y = qty, for whatever your input to qty is.\nThe above then says, on the x-axis, we want a line from the district - 0.5 to the district + 0.5, while we set yend = e_dvs to match the implicitly set y = e_dvs, since we passed qty = e_dvs before.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = ggplot2::geom_violin, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nThe good thing here is that we can adjust the ref_geom however we see fit at this point. So if that red line is too dark, but also too skinny, we can do something like changing the alpha:\n\n\nr_geom <- function(...) \n    ggplot2::geom_segment(\n        ggplot2::aes(x = as.integer(.distr_no) - 0.5,\n                     xend = as.integer(.distr_no) + 0.5,\n                     yend = e_dvs,\n                     color = .data$draw),\n        linewidth = 1, alpha = 0.7,\n        ...\n    )\n\n\nThen this fixes those particular issues.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = ggplot2::geom_violin, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nNow, there are tons of other things we can do here. If we want to revisit the 95% confidence interval issue, we can turn to ggdist.\n\n\nlibrary(ggdist)\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = stat_pointinterval, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nNow, we have really clear idea of how wide the 95% confidence interval goes (via the length of the skinny lines).\nAnd really, the sky is the limit with packages like ggdist. For example, if we want a raincloud, we can do that.\n\n\nraincloud <- function(...) {\n    list(\n        ggdist::stat_slab(aes(thickness = ggplot2::after_stat(pdf*n)), scale = 1),\n        ggdist::stat_dotsinterval(side = \"bottom\", scale = 1,\n                                  slab_size = NA, quantiles = 100)\n    )\n}\n\n\nThis gives us a fun plot to work with, though this might be best suited for much larger plot areas.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = raincloud, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.35, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nBetter Diagnostics for summary.redist_plans()\nLike above, let’s get some simulated plans from the 50-State Redistricting Simulations. We can get a state like Nevada, which has fewer districts and shorter summary.\n\n\nlibrary(alarmdata)\nplans <- alarm_50state_plans('NV')\n\n\nTo get diagnostics, we can call summary(plans) which computes R-hats, sample diversity, and some split-by-split SMC diagnostics.\n\n\nsummary(plans)\n\n\n\nR-hat values for summary statistics:\n   pop_overlap      total_vap       plan_dev      comp_edge \n         1.003          1.003          1.001          1.000 \n   comp_polsby       pop_hisp      pop_white      pop_black \n         1.000          1.009          1.005          1.003 \n      pop_aian      pop_asian       pop_nhpi      pop_other \n         1.002          1.003          1.000          1.003 \n       pop_two       vap_hisp      vap_white      vap_black \n         1.003          1.007          1.006          1.004 \n      vap_aian      vap_asian       vap_nhpi      vap_other \n         1.001          1.002          1.000          1.007 \n       vap_two pre_16_dem_cli pre_16_rep_tru uss_16_dem_cor \n         1.003          1.001          1.003          1.000 \nuss_16_rep_hec uss_18_dem_ros uss_18_rep_hel gov_18_dem_sis \n         1.004          1.002          1.002          1.002 \ngov_18_rep_lax atg_18_dem_for atg_18_rep_dun sos_18_dem_ara \n         1.003          1.001          1.002          1.001 \nsos_18_rep_ceg pre_20_dem_bid pre_20_rep_tru         arv_16 \n         1.003          1.002          1.004          1.003 \n        adv_16         arv_18         adv_18         arv_20 \n         1.000          1.003          1.002          1.004 \n        adv_20  county_splits    muni_splits            ndv \n         1.002          1.002          1.003          1.002 \n           nrv        ndshare          e_dvs         pr_dem \n         1.003          1.003          1.003          1.001 \n         e_dem          pbias           egap \n         1.002          1.000          1.001 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,215 (88.6%)     19.7%        0.93 1,583 (100%)     10 \nSplit 2     2,287 (91.5%)     12.8%        0.51 1,565 ( 99%)      6 \nSplit 3     2,242 (89.7%)      5.9%        0.56 1,441 ( 91%)      4 \nResample    1,352 (54.1%)       NA%        0.57 1,409 ( 89%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,228 (89.1%)     15.1%        0.91 1,584 (100%)     13 \nSplit 2     2,285 (91.4%)      9.7%        0.52 1,574 (100%)      8 \nSplit 3     2,236 (89.4%)      5.0%        0.58 1,444 ( 91%)      5 \nResample    1,471 (58.8%)       NA%        0.59 1,399 ( 89%)     NA \n\nThe first big change here is that the digits are now rounded to three digits.\nYou no longer need to search through 8 decimal digits at 3am for the ones that matter.\nTypically, we want R-hat values between 1 and 1.05, so this looks pretty good.\nWhat if they weren’t? We can introduce this behavior by adding some new variable with very different values by independent run of SMC (denoted by the chain column).\n\n\nplans <- plans %>% \n    mutate(bad_rhat = rnorm(n = n(), mean = dplyr::coalesce(chain, 0)))\n\n\nNow this gets angry:\n\n\nsummary(plans)\n\n\n\nR-hat values for summary statistics:\n   pop_overlap      total_vap       plan_dev      comp_edge \n         1.003          1.003          1.001          1.000 \n   comp_polsby       pop_hisp      pop_white      pop_black \n         1.000          1.009          1.005          1.003 \n      pop_aian      pop_asian       pop_nhpi      pop_other \n         1.002          1.003          1.000          1.003 \n       pop_two       vap_hisp      vap_white      vap_black \n         1.003          1.007          1.006          1.004 \n      vap_aian      vap_asian       vap_nhpi      vap_other \n         1.001          1.002          1.000          1.007 \n       vap_two pre_16_dem_cli pre_16_rep_tru uss_16_dem_cor \n         1.003          1.001          1.003          1.000 \nuss_16_rep_hec uss_18_dem_ros uss_18_rep_hel gov_18_dem_sis \n         1.004          1.002          1.002          1.002 \ngov_18_rep_lax atg_18_dem_for atg_18_rep_dun sos_18_dem_ara \n         1.003          1.001          1.002          1.001 \nsos_18_rep_ceg pre_20_dem_bid pre_20_rep_tru         arv_16 \n         1.003          1.002          1.004          1.003 \n        adv_16         arv_18         adv_18         arv_20 \n         1.000          1.003          1.002          1.004 \n        adv_20  county_splits    muni_splits            ndv \n         1.002          1.002          1.003          1.002 \n           nrv        ndshare          e_dvs         pr_dem \n         1.003          1.003          1.003          1.001 \n         e_dem          pbias           egap       bad_rhat \n         1.002          1.000          1.001        ❌1.224 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,215 (88.6%)     19.7%        0.93 1,583 (100%)     10 \nSplit 2     2,287 (91.5%)     12.8%        0.51 1,565 ( 99%)      6 \nSplit 3     2,242 (89.7%)      5.9%        0.56 1,441 ( 91%)      4 \nResample    1,352 (54.1%)       NA%        0.57 1,409 ( 89%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,228 (89.1%)     15.1%        0.91 1,584 (100%)     13 \nSplit 2     2,285 (91.4%)      9.7%        0.52 1,574 (100%)      8 \nSplit 3     2,236 (89.4%)      5.0%        0.58 1,444 ( 91%)      5 \nResample    1,471 (58.8%)       NA%        0.59 1,399 ( 89%)     NA \n\nIt warns about convergence, as it has since 4.0. But it now also adds a big red “x” next to bad_rhat’s R-hat.\nAny questions? Open an issue on GitHub or find us on Twitter.\n\n\n\n",
    "preview": "posts/2023-03-18-redist-41/redist-41_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2023-05-01T15:48:15-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-01-31-comment-the-essenital-role-of-policy-evaluation/",
    "title": "Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System",
    "description": "Our response to boyd and Sarathy (2022) is now published in the HDSR!",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2023-01-31",
    "categories": [],
    "contents": "\nWe’re excited to share that Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System is now available at the Harvard Data Science Review. We discuss boyd and Sarathy (2022), addressing both factual inaccuracies in their work and their contention that disagreements over privacy in the 2020 Census are primarily academic issues.\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the U.S. Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue that empirical evaluations of the Census Disclosure Avoidance System (DAS), including our published analysis (Kenny et al., 2021b), failed to recognize that the benchmark data against which the 2020 DAS was evaluated is never a ground truth of population counts. In this commentary, we explain why policy evaluation, which was the main goal of our analysis, is still meaningful without access to a perfect ground truth. We also point out that our evaluation leveraged features specific to the decennial census and redistricting data, such as block-level population invariance under swapping and voter file racial identification, better approximating a comparison with the ground truth. Lastly, we show that accurate statistical predictions of individual race based on the Bayesian Improved Surname Geocoding, while not a violation of differential privacy, substantially increases the disclosure risk of private information the Census Bureau sought to protect. We conclude by arguing that policymakers must confront a key trade-off between data utility and privacy protection, and an epistemic disconnect alone is insufficient to explain disagreements between policy choices.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-15-statistical-software-award/",
    "title": "redist Recieved POLMETH's 2022 Statistical Software Award",
    "description": "Our software won the Society for Political Methodology's Statistical Software Award.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Ben Fifield",
        "url": "https://www.benfifield.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-12-15",
    "categories": [],
    "contents": "\nLast week, Chris Kenny, Cory McCartan, Ben Fifield, and Kosuke Imai\nreceived this year’s Statistical Software Award from the Society for\nPolitical Methodology for the R package redist.\nThe announcement stated:\n\nThe redist package by Christopher T Kenny, Cory\nMcCartan, Ben Fifield, and Kosuke Imai of the Algorithm-Assisted\nRedistricting Methodology Project has rapidly become a key resource for\nresearchers and practitioners seeking to evaluate redistricting plans.\nredist develops statistically grounded and computationally\nefficient procedures for generating random draws from a distribution of\nviable redistricting plans, including conditional distributions that\nsatisfy specified requirements for geographic compactness and population\nparity. The package allows users to test for illegal partisan and racial\ngerrymandering, a timely and important question in the wake of the 2020\nCensus and the redistricting cycle that followed. It has had a\nsubstantial policy impact seeing use in legal challenges against and,\nunusually, has also been cited by six Supreme Court justices in oral\narguments. In short, it is an ideal recipient of the Society for\nPolitical Methodolgy’s 2022 Statistical Software Award.\n\nWe are grateful for the honor! Thank you to the award committee for\nconsidering our software.\nredist is an open source R package for sampling\nredistricting plans, available here:\n\nThis R package enables researchers to sample redistricting plans from\na pre-specified target distribution using Sequential Monte Carlo and\nMarkov Chain Monte Carlo algorithms. The package supports various\nconstraints in the redistricting process, such as geographic compactness\nand population parity requirements. Tools for analysis, including\ncomputation of various summary statistics and plotting functionality,\nare also included.\n\nredist is a key tool for much of our work, which has\nenabled work on identifying\nbias in Census 2020, creating\nalternative plans for all 50 states, reducing\nmalapportionment in Japan, assessing\npartisan bias in 2022’s congressional districts, and more.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/alarm-redist/redist/main/man/figures/logo.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-01-50statessimulations/",
    "title": "50statesSimulations in Nature Scientific Data",
    "description": "Now published at Nature Scientific Data.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "George Garcia III",
        "url": {}
      },
      {
        "name": "Kevin Wang",
        "url": "https://scholar.harvard.edu/kevinwang"
      },
      {
        "name": "Melissa Wu",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-11-14",
    "categories": [],
    "contents": "\nWe’re excited to share that our article, Simulated redistricting\nplans for the analysis and evaluation of redistricting in the United\nStates, is now available through Nature\nScientific Data. This is a great step in making alternative\nredistricting plans more available for the public and researchers.\nCheck out the alarmdata R\npackage for some helpers for working with this data!\nWant more information? Take a look at the\nblog post that accompanied the release of the original working paper\nor an\nearly use of this data.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-13-policy-evaluation-commentary/",
    "title": "Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System",
    "description": "We're excited to announce our forthcoming article discussing boyd and Sarathy (2022).",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2022-10-18",
    "categories": [],
    "contents": "\nWe’re excited to announce a new paper, Comment: The Essential Role of\nPolicy Evaluation for the 2020 Census Disclosure Avoidance System,\nnow forthcoming at the Harvard Data Science Review. This\narticle responds to boyd and\nSarathy (2022). This builds on our research into\nthe impact of differential privacy on redistricting and advances our\nresponses\nto some common questions.\nThe abstract is listed below:\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the\nUS Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue\nthat empirical evaluations of the Census Disclosure Avoidance System\n(DAS), including our published analysis, failed to recognize how the\nbenchmark data against which the 2020 DAS was evaluated is never a\nground truth of population counts. In this commentary, we explain why\npolicy evaluation, which was the main goal of our analysis, is still\nmeaningful without access to a perfect ground truth. We also point out\nthat our evaluation leveraged features specific to the decennial Census\nand redistricting data, such as block-level population invariance under\nswapping and voter file racial identification, better approximating a\ncomparison with the ground truth. Lastly, we show that accurate\nstatistical predictions of individual race based on the Bayesian\nImproved Surname Geocoding, while not a violation of differential\nprivacy, substantially increases the disclosure risk of private\ninformation the Census Bureau sought to protect. We conclude by arguing\nthat policy makers must confront a key trade-off between data utility\nand privacy protection, and an epistemic disconnect alone is\ninsufficient to explain disagreements between policy choices.\n\nFor those interested in the full commentary, a preprint is available\nhere.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-30-nikkei/",
    "title": "'One vote disparity' can be improved with state-of-the-art algorithms",
    "description": "Our article in Nikkei Business on reducing Japanese malapportionment was released!",
    "author": [
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      },
      {
        "name": "Kento Yamada",
        "url": {}
      },
      {
        "name": "Rei Yatsuhashi",
        "url": {}
      },
      {
        "name": "Sho Miyazaki",
        "url": {}
      }
    ],
    "date": "2022-09-30",
    "categories": [],
    "contents": "\nALARMプロジェクトは、日経ビジネスの依頼を受け、衆議院の区割り改定案の作成とその分析の記事を寄稿しました。区割りシミュレーションアルゴリズムの活用により、区画審の改定案よりも、市区町村の分割数と一票の格差が少なくなる区割り案が作成できます。https://business.nikkei.com/atcl/gen/19/00351/092100048/\nWe were invited to contribute to Nikkei Business (Japan)! We show\nthat the SMC redistricting simulation algorithm can be used to reduce\nmalapportionment without splitting more municipalities, when compared to\nthe districting plan proposed by the commission. https://business.nikkei.com/atcl/gen/19/00351/092100048/\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-14-gerrymandering-mostly-cancels-nationally/",
    "title": "Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition",
    "description": "Gerrymandering in 2020 redistricting makes the US House elections less \ncompetitive, but net seat gains are small nationally. The partisan bias of the\nenacted national map is about as biased as non-partisan simulations, due to\ngeography and legal requirements.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-08-16",
    "categories": [],
    "contents": "\nWe’re excited to release a\nnew working paper studying partisan bias in the 2020 US House plan.\nWe employ redistricting simulations from the 50stateSimulations.1 with a model of partisanship to dig\ninto geographic details of what happened where. Gerrymandering in\n2020 redistricting makes the US House elections less competitive, but\nnet seat gains are small nationally. The abstract below highlights\nmore of our findings.\n\nCongressional district lines in many U.S. states are drawn by\npartisan actors, raising concerns about gerrymandering. To isolate the\nelectoral impact of gerrymandering from the effects of other factors\nincluding geography and redistricting rules, we compare predicted\nelection outcomes under the enacted plan with those under a large sample\nof non-partisan, simulated alternative plans for all states. We find\nthat partisan gerrymandering is widespread in the 2020 redistricting\ncycle, but most of the bias it creates cancels at the national level,\ngiving Republicans two additional seats, on average. In contrast,\nmoderate pro-Republican bias due to geography and redistricting rules\nremains. Finally, we find that partisan gerrymandering reduces electoral\ncompetition and makes the House’s partisan composition less responsive\nto shifts in the national vote.\n\nAll in all, most of the states you thought were gerrymandered are\nindeed gerrymandered.\nSome gerrymandered states that you might have missed include those\ndrawn by commissions (like Michigan and Iowa) and those drawn by courts\n(like Pennsylvania and North Carolina). The state-by-state results are\nbelow, with a national topline that the US map favors Republicans by\nabout two seats beyond what’s explained by geography, on average.\n\n\n\nWe can take this further and look at a partisan manipulation\nmap. Each district is colored by the difference in probability that\nit is represented by a Republican or Democrat in the enacted from the\nsimulated plans. Red areas favor Republicans over simulations, blue\nareas favor Democrats over simulations. The darkness of each district\nrepresents the intensity of that difference.\n\n\n\nIf you’re interested in more information on our findings or methods,\ntake a look here.\nA special thank you to George Garcia III, Kevin Wang, and Melissa Wu\nfor their contributions to the 50stateSimulations which\nmade this research possible.\n\nIn case you missed it, we\nhave a blog post introducing those simulations and their\ncontributors.↩︎\n",
    "preview": "posts/2022-08-14-gerrymandering-mostly-cancels-nationally/state_sum_1.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 3300
  },
  {
    "path": "posts/2022-06-23-fifty-states-data-descriptor/",
    "title": "Fifty States Data Descriptor",
    "description": "A detailed description of the 50-State Redistricting Simulations \nand new software to help you use them.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "George Garcia III",
        "url": {}
      },
      {
        "name": "Kevin Wang",
        "url": "https://scholar.harvard.edu/kevinwang"
      },
      {
        "name": "Melissa Wu",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-07-28",
    "categories": [],
    "contents": "\nIt’s been a long redistricting year. We’ve been\ntracking passed maps while conducting simulations in the 44 states with\ncongressional districts. We are now finalizing some re-runs of states\nwith new validation steps based on additional diagnostics, to ensure a\nhigh quality and accurate data product. So, we’ve written up a more\ndetailed draft of what we did, how we did it, and how we checked our\nwork. Most importantly, it introduces some tools so that you can use the\ndata we’ve generated. It’s all open source and the redistricting plans\ngenerated are in the public domain.\nRead the detailed description of the our process and the data: Simulated redistricting plans\nfor the analysis and evaluation of redistricting in the United States:\n50stateSimulations. The abstract is listed below.\n\nThis article introduces the 50stateSimulations, a collection of\nsimulated congressional districting plans and underlying code developed\nby the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The\n50stateSimulations allow for the evaluation of enacted and other\ncongressional redistricting plans in the United States. While the use of\nredistricting simulation algorithms has become standard in academic\nresearch and court cases, any simulation analysis requires non-trivial\nefforts to combine multiple data sets, identify state-specific\nredistricting criteria, implement complex simulation algorithms, and\nsummarize and visualize simulation outputs. We have developed a complete\nworkflow that facilitates this entire process of simulation-based\nredistricting analysis for the congressional districts of all 50 states.\nThe resulting 50stateSimulations include ensembles of simulated 2020\ncongressional redistricting plans and necessary replication data. We\nalso provide the underlying code, which serves as a template for\ncustomized analyses. All data and code are free and publicly available.\nThis article details the design, creation, and validation of the\ndata.\n\nTo help make things more usable for those who don’t simulate\nredistricting plans in their free time, we are also excited to (soft)\nlaunch a new R package, alarmdata.\nThis package provides a simplified interface to download the underlying\ngeographic data, generated plans, all sorts of summary statistics, and\nstate-by-state documentation.\nThe package can be installed with:\n\n\nremotes::install_github('alarm-redist/alarmdata')\n\n\nThank you to the Harvard Data Science Initiative and Microsoft for\ncomputational support.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-06-japanese-society-of-quantitative-political-science/",
    "title": "47-Prefectures at the Japanese Society of Quantitative Political Science",
    "description": "We are presenting Friday on malapportionment. 私たちは、アルゴリズムを用いた一票の格差の是正について、金曜日に発表します。",
    "author": [
      {
        "name": "Sho Miyazaki",
        "url": {}
      },
      {
        "name": "Kento Yamada",
        "url": {}
      },
      {
        "name": "Rei Yatsuhashi",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2022-07-07",
    "categories": [],
    "contents": "\nWe are presenting a poster presentation at the Japanese Society of\nQuantitative Political Science 🇯🇵! We show that the well-known\nmalapportionment (一票の格差) of the Japanese House of Representatives\ncan be redressed without splitting administrative boundaries.\nALARMプロジェクトは、計量・数理政治研究会（JSQPS）の夏季集会にてポスターセッションに参加致します。アルゴリズムを衆議院の選挙区改変に応用して、市区町村の分割数を増やさなくても、一票の格差が是正される区割り案を作成できることを発表します。\nView\nthe poster! Or\nsee the full details of the project.\nポスターをご覧ください！\n詳細は、本プロジェクトのウェブサイトをご確認ください。\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-20-redist-40/",
    "title": "redist 4.0",
    "description": "A major release with big changes to constraints and diagnostics.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      }
    ],
    "date": "2022-06-20",
    "categories": [],
    "contents": "\nWe are excited to announce the arrival of redist 4.0.1\non CRAN. This\nupdate focuses on increasing constraint consistency and diagnostic\nusability. The new tools here have been thoroughly tested as part of the\n50-State\nRedistricting Simulations project.\nTo install the new version, run\ninstall.packages('redist').\nNew Features\nA new constraint interface that is more flexible, user friendly, and\nconsistent across algorithms (see redist_constr() and\n?constraints). For the first time, user-defined custom\nconstraints are supported and integrated within all three\nalgorithms.\nNew diagnostic-checking function,\nsummary.redist_plans()\nSummary statistics have been broken out into a new\nredistmetrics package This will speed up compilation time\nand also provides a cleaner, more extensible interface for the\nimplementation of additional metrics.\nParallel computing support for the SMC algorithm, both within and\nacross sampling runs\nReproducible across-run parallelism throughout the package, via\ndoRNG\nMuch faster match_numbers() using the Hungarian\nmethod\nmin_move_parity() calculates how much population needs\nto be moved between districts in order to completely balance a\nredistricting plan.\nSupport for partial SMC simulations, where fewer districts are drawn\nthan the total number. Allows advanced users to manually combine partial\nruns to form complete maps.\nImproved algorithm reporting, including new progress bars and\ncli errors and warnings throughout the package\nUpdate the SMC algorithm to include a missing correction factor for\nthe number of ways to sequentially label districts. This factor should\nnot have an effect on substantive conclusions and summary\nstatistics.\nRemove deprecated functions\nMany bug fixes (see https://github.com/alarm-redist/redist/issues)\nUpdated Features: A Brief\nDemo\nThe first thing you’ll notice upon loading redist is\nthat it also loads redistmetrics.\nredistmetrics used to live within redist but\nhas been separated to keep the package size reasonable and to make the\nindividual compile times shorter.\n\n\nlibrary(tidyverse)\nlibrary(redist)\n\n\n\nWe can pull in some\ndata from the ALARM Project, which combines 2020 Census data with VEST’s\nelection data, retabulated to 2020 voting districts. For this example,\nwe can use data from New Mexico.\n\n\nnm <- geomander::get_alarm('NM')\n\n\n\n\nWe can then make a redist_map for New Mexico.1\n\n\nmap_nm <- redist_map(nm, ndists = 3, pop_tol = 0.005)\n\n\n\nAnd we can begin with a basic run of redist_smc to\nsample 1000 plans using the sampler from Sequential Monte Carlo for\nSampling Balanced and Compact Redistricting Plans by Cory McCartan and Kosuke Imai. Most importantly,\nredist_smc now offers an argument for the number of\nindependent sampling runs. For now, we can break that 1000 plans into 2\nruns of 500.\n\n\nset.seed(2022)\nplans <- redist_smc(map = map_nm, nsims = 500, runs = 2, counties = county)\n\n\n\nThe new messages above are created with cli to more make\nmessage printing cleaner and more consistent.\nTo the output, we can add some basic summary information using\navailable redistmetrics functions, automatically loaded by\nredist.\n\n\nplans <- plans %>% \n    mutate(\n        frac_kept = comp_frac_kept(plans = ., shp = map_nm),\n        dvs_gov_18 = part_dvs(plans = ., shp = map_nm, dvote = gov_18_dem_luj, rvote = gov_18_rep_pea),\n        county_spl = splits_admin(plans = ., shp = map_nm, admin = county)\n    )\n\n\n\nIn order, this adds the Fraction Kept compactness score, the\nDemocratic two-party vote share in the 2018 Governor’s race, and the\nnumber of counties split.\nNow, the plans object has a few new columns:\n\n\nhead(plans)\n\n\nWith plans resampled from weights\nPlans matrix: int [1:1977, 1:1000] 3 3 3 3 3 3 3 3 3 3 ...\n# A tibble: 6 × 7\n  draw  chain district total_pop frac_kept dvs_gov_18 county_spl\n  <fct> <int>    <int>     <dbl>     <dbl>      <dbl>      <int>\n1 1         1        1    706220     0.984      0.474          2\n2 1         1        2    707900     0.984      0.602          2\n3 1         1        3    703402     0.984      0.619          2\n4 2         1        1    707157     0.990      0.508          2\n5 2         1        2    706801     0.990      0.582          2\n6 2         1        3    703564     0.990      0.616          2\n\nDraw, chain, and district identify each plan, where\nchain is new to 4.0 for SMC. It signifies the SMC run,\nsimilar to how redist_mergesplit_parallel indicates the\nchain from merge-split. Despite this, we can use the normal plotting\nfunctions on the redist_plans object. If we load\npatchwork here to get a nice row of ggplots,\nwe see the following:\n\n\nlibrary(patchwork)\nhist(plans, frac_kept) + \n    plot(plans, dvs_gov_18) + \n    hist(plans, county_spl)\n\n\n\n\nThese plots are fairly standard. The exciting thing is that we can\nnow call summary() to get diagnostic information about the\nruns of SMC. We can call this on any redist_plans object\nand it will adjust the output information depending on what algorithm\ngenerated the plans.\n\n\nsummary(plans)\n\n\n\n\nR-hat values for summary statistics:\n frac_kept dvs_gov_18 county_spl \n   1.00259    1.00197    0.99955 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       485 (97.0%)      9.9%        0.38   469 ( 94%)     10 \nSplit 2       474 (94.9%)      5.3%        0.48   397 ( 79%)      6 \nResample      411 (82.2%)       NA%        0.48   417 ( 83%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       482 (96.5%)     12.2%        0.41   471 ( 94%)      8 \nSplit 2       476 (95.3%)      6.7%        0.46   405 ( 81%)      5 \nResample      417 (83.4%)       NA%        0.45   419 ( 84%)     NA \n\nEach R-hat value is below 1.05, so we do not get any warnings. At a\nhigh level, this means that both runs of SMC are sampling from regions\ncomparable by these three summary statistics. That isn’t always the case\nthough. If you do get a warning, you should increase the number of\nsimulations or decrease the constraint strengths.\nWe next introduce the new constraint interface. To initialize a\nconstraint, we call redist_constr, which takes a\nredist_map input.\n\n\nconstr <- redist_constr(map = map_nm)\n\n\n\nWe can add any of the many constraints available with\n?constraints. There are many new constraints to people who\nhave only used redist_smc/redist_mergesplit or\nredist_flip before. Now all constraints are available to\nall algorithms. Additionally, we can write pretty much any constraint\nthat we can map to the positive reals, using the new custom\nconstraint.\nFor our custom constraint, we just care that the 100th row of\nmap_nm won’t be assigned to district 3. We can do the\nfollowing\n\n\nconstr <- constr %>% \n    add_constr_custom(\n        strength = 10,\n        fn = function(plan, distr) {\n            as.numeric(plan[100] != 3)\n        }\n    )\n\n\n\nThis takes an R function fn and a strength value (how\nmuch to multiply the output of fn by). The fn\ninput should always take the form\nfunction(plan, distr) { ... }, where plan will\nbe an integer matrix of precinct-district assignments and\ndistr will be the current district.\nWe can then pass constr to the constraints\nargument in redist_smc().\n\n\nset.seed(2022)\nplans <- redist_smc(map = map_nm, nsims = 500, runs = 2, counties = county, \n                    constraints = constr)\n\n\n\nAgain, we add some summary statistics.\n\n\nplans <- plans %>% \n    mutate(\n        frac_kept = comp_frac_kept(plans = ., shp = map_nm),\n        dvs_gov_18 = part_dvs(plans = ., shp = map_nm, dvote = gov_18_dem_luj, rvote = gov_18_rep_pea),\n        county_spl = splits_admin(plans = ., shp = map_nm, admin = county)\n    )\n\n\n\nThen run the diagnostics:\n\n\nsummary(plans)\n\n\n\n\nR-hat values for summary statistics:\n frac_kept dvs_gov_18 county_spl \n   0.99916    1.00000    1.00845 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       484 (96.8%)     10.9%        0.38   475 ( 95%)     10 \nSplit 2       476 (95.1%)      5.7%        0.46   399 ( 80%)      6 \nResample      413 (82.6%)       NA%        0.46   407 ( 81%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       483 (96.6%)     12.8%        0.40   478 ( 96%)      8 \nSplit 2       477 (95.4%)      7.3%        0.45   388 ( 78%)      5 \nResample      419 (83.8%)       NA%        0.45   420 ( 84%)     NA \n\nAnd everything looks good. Despite adding a constraint, the sample\nstill looks fine under these summary statistics.\nFor more information on diagnostics, take a look at McCartan and Imai\n(2022).\n\nFor a very brief intro to\nredist_maps, see the 3.0 release post at https://alarm-redist.github.io/posts/2021-04-02-redist-300/.↩︎\n",
    "preview": "posts/2022-06-20-redist-40/redist-40_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-23-47-prefecture-project/",
    "title": "47-Prefecture Project",
    "description": "Using redistricting simulation methods to better understand redistricting in Japan.",
    "author": [
      {
        "name": "Sho Miyazaki",
        "url": {}
      },
      {
        "name": "Kento Yamada",
        "url": {}
      },
      {
        "name": "Rei Yatsuhashi",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2022-04-23",
    "categories": [],
    "contents": "\nThe ALARM Project is pleased to announce the 47-Prefecture Project, whose goal is to generate and analyze redistricting plans for the single-member districts of the House of Representatives of Japan using redistricting simulation algorithms.\n\nLearn more about the project and see the results »\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-06-2021-10-06-das-published/",
    "title": "Revised and published: The use of differential privacy for census data and its impact on redistricting",
    "description": "A new postscript analyzes the final version of the U.S. Census Bureau's Disclosure Avoidance System.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nAt the end of May, the ALARM Project released a report examining “The Impact U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis.” The Census Bureau has since updated the system parameters and released 2020 Census Data protected under this Disclosure Avoidance System.\nOur original report has been revised to include an analysis of the new system parameters, and appears today in the journal Science Advances.\nRead the paper: The use of differential privacy for census data and its impact on redistricting: The case of the 2020 U.S. Census\n\n\n\n\n",
    "preview": "https://pbs.twimg.com/media/FA8fplwXsAMVffB?format=jpg&name=large",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-census-2020/",
    "title": "2020 Redistricting Data Files",
    "description": "Census and election data joined together for use in redistricting\nand voting rights analysis.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\n\nAKALARAZCACOCTDEFLGAHIIAIDILINKSKYLAMAMDMEMIMNMOMSMTNCNDNENHNJNMNVNYOHOKORPARISCSDTNTXUTVAVTWAWIWVWYDCUSA\n\nThe ALARM Project is glad to provide precinct-level demographic and election data\nfrom the 2020 decennial census and the\nVoting and Election Science Team\nwhich have been tidied and joined together using 2020 precinct boundaries.\nWhere 2020 precinct boundaries are not available, Census block-level data is\nprovided instead, and where no VEST data is available, only demographic\ninformation is provided. Code to generate the data from these sources is\nincluded; the entire workflow is open-source and reproducible.\nGetting the data\nDownload individual states’ data below, or\ndownload a ZIP of all the data here.\nOur repository also contains\nmore detailed data, as well as code and instructions for programmatic\ndownloading, adding shapefile geometries, and other use cases.\nPlease make sure to cite the\nVoting and Election Science Team\nand the U.S. Census Bureau.\nConsult the license\nfor information on modifying and sharing the data and/or code.\n2020 state data\n\n\n\n\n \n\nAlabama\n \nVTDs\n\n \n\n \nal_2020_vtd.csv\n\n\n\n\n\n \n\nAlaska\n \nVTDs\n\n \n\n \nak_2020_vtd.csv\n\n\n\n\n\n \n\nArizona\n \nVTDs\n\n \n\n \naz_2020_vtd.csv\n\n\n\n\n\n \n\nArkansas\n \nVTDs\n\n \n\n \nar_2020_vtd.csv\n\n\n\n\n\n \n\nCalifornia\n \nCensus blocks\n\n \n\n \nca_2020_block.csv\n\n\n\n\n\n \n\nColorado\n \nVTDs\n\n \n\n \nco_2020_vtd.csv\n\n\n\n\n\n \n\nConnecticut\n \nVTDs\n\n \n\n \nct_2020_vtd.csv\n\n\n\n\n\n \n\nDelaware\n \nVTDs\n\n \n\n \nde_2020_vtd.csv\n\n\n\n\n\n \n\nDistrict of Columbia\n \nVTDs\n\n \n\n \ndc_2020_vtd.csv\n\n\n\n\n\n \n\nFlorida\n \nVTDs\n\n \n\n \nfl_2020_vtd.csv\n\n\n\n\n\n \n\nGeorgia\n \nVTDs\n\n \n\n \nga_2020_vtd.csv\n\n\n\n\n\n \n\nHawaii\n \nCensus blocks\n\n \n\n \nhi_2020_block.csv\n\n\n\n\n\n \n\nIdaho\n \nVTDs\n\n \n\n \nid_2020_vtd.csv\n\n\n\n\n\n \n\nIllinois\n \nVTDs\n\n \n\n \nil_2020_vtd.csv\n\n\n\n\n\n \n\nIndiana\n \nVTDs\n\n \n\n \nin_2020_vtd.csv\n\n\n\n\n\n \n\nIowa\n \nVTDs\n\n \n\n \nia_2020_vtd.csv\n\n\n\n\n\n \n\nKansas\n \nVTDs\n\n \n\n \nks_2020_vtd.csv\n\n\n\n\n\n \n\nKentucky\n \nVTDs\n\n \n\n \nky_2020_vtd.csv\n\n\n\n\n\n \n\nLouisiana\n \nVTDs\n\n \n\n \nla_2020_vtd.csv\n\n\n\n\n\n \n\nMaine\n \nVTDs\n\n \n\n \nme_2020_vtd.csv\n\n\n\n\n\n \n\nMaryland\n \nVTDs\n\n \n\n \nmd_2020_vtd.csv\n\n\n\n\n\n \n\nMassachusetts\n \nVTDs\n\n \n\n \nma_2020_vtd.csv\n\n\n\n\n\n \n\nMichigan\n \nVTDs\n\n \n\n \nmi_2020_vtd.csv\n\n\n\n\n\n \n\nMinnesota\n \nVTDs\n\n \n\n \nmn_2020_vtd.csv\n\n\n\n\n\n \n\nMississippi\n \nVTDs\n\n \n\n \nms_2020_vtd.csv\n\n\n\n\n\n \n\nMissouri\n \nVTDs\n\n \n\n \nmo_2020_vtd.csv\n\n\n\n\n\n \n\nMontana\n \nVTDs\n\n \n\n \nmt_2020_vtd.csv\n\n\n\n\n\n \n\nNebraska\n \nVTDs\n\n \n\n \nne_2020_vtd.csv\n\n\n\n\n\n \n\nNevada\n \nVTDs\n\n \n\n \nnv_2020_vtd.csv\n\n\n\n\n\n \n\nNew Hampshire\n \nVTDs\n\n \n\n \nnh_2020_vtd.csv\n\n\n\n\n\n \n\nNew Jersey\n \nVTDs\n\n \n\n \nnj_2020_vtd.csv\n\n\n\n\n\n \n\nNew Mexico\n \nVTDs\n\n \n\n \nnm_2020_vtd.csv\n\n\n\n\n\n \n\nNew York\n \nVTDs\n\n \n\n \nny_2020_vtd.csv\n\n\n\n\n\n \n\nNorth Carolina\n \nVTDs\n\n \n\n \nnc_2020_vtd.csv\n\n\n\n\n\n \n\nNorth Dakota\n \nVTDs\n\n \n\n \nnd_2020_vtd.csv\n\n\n\n\n\n \n\nOhio\n \nVTDs\n\n \n\n \noh_2020_vtd.csv\n\n\n\n\n\n \n\nOklahoma\n \nVTDs\n\n \n\n \nok_2020_vtd.csv\n\n\n\n\n\n \n\nOregon\n \nCensus blocks\n\n \n\n \nor_2020_block.csv\n\n\n\n\n\n \n\nPennsylvania\n \nVTDs\n\n \n\n \npa_2020_vtd.csv\n\n\n\n\n\n \n\nRhode Island\n \nVTDs\n\n \n\n \nri_2020_vtd.csv\n\n\n\n\n\n \n\nSouth Carolina\n \nVTDs\n\n \n\n \nsc_2020_vtd.csv\n\n\n\n\n\n \n\nSouth Dakota\n \nVTDs\n\n \n\n \nsd_2020_vtd.csv\n\n\n\n\n\n \n\nTennessee\n \nVTDs\n\n \n\n \ntn_2020_vtd.csv\n\n\n\n\n\n \n\nTexas\n \nVTDs\n\n \n\n \ntx_2020_vtd.csv\n\n\n\n\n\n \n\nUtah\n \nVTDs\n\n \n\n \nut_2020_vtd.csv\n\n\n\n\n\n \n\nVermont\n \nVTDs\n\n \n\n \nvt_2020_vtd.csv\n\n\n\n\n\n \n\nVirginia\n \nVTDs\n\n \n\n \nva_2020_vtd.csv\n\n\n\n\n\n \n\nWashington\n \nVTDs\n\n \n\n \nwa_2020_vtd.csv\n\n\n\n\n\n \n\nWest Virginia\n \nVTDs\n\n \n\n \nwv_2020_vtd.csv\n\n\n\n\n\n \n\nWisconsin\n \nVTDs\n\n \n\n \nwi_2020_vtd.csv\n\n\n\n\n\n \n\nWyoming\n \nVTDs\n\n \n\n \nwy_2020_vtd.csv\n\n\n\n\nUsing the data\nData Format\nEach data table contains several identification columns, a set of census-derived\ndemographic columns, and a set of VEST-derived election columns.\nGEOID20 is the unique identifier for a precinct or Census block.\nThe state and county of the precinct or block are also provided.\nCensus variables are prefixed with pop_ or vap_, depending on whether\nthey are for the entire population or the voting-age population.\nSuffixes refer to racial and ethnic categories, as follows:\n_hisp: Hispanic or Latino (of any race)\n_white: White alone, not Hispanic or Latino\n_black: Black or African American alone, not Hispanic or Latino\n_aian: American Indian and Alaska Native alone, not Hispanic or Latino\n_asian: Asian alone, not Hispanic or Latino\n_nhpi: Native Hawaiian and Other Pacific Islander alone, not Hispanic or Latino\n_other: Some Other Race alone, not Hispanic or Latino\n_two: Population of two or more races, not Hispanic or Latino\n\nElection variables consist of average vote counts for Democratic and\nRepublican candidates. The adv_## and arv_## columns report the\naverage vote count in the ## election, across all statewide races\ncontested by both parties. The ndv and nrv columns further average\nthe vote counts across all available election years. For specific statewide\nraces, you may download the files in vest-2020/ and join them to the data\nusing the GEOID20 column.\nMore Tools\nFor redistricting and voting rights analysis, we recommend the\nredist package.\nFor pre-processing and tidying data for redistricting analysis, we recommend the\ngeomander package.\nFor more custom tabulations of the 2020 census data, we recommend the\nPL94171 package.\nFor general-purpose census data processing, we recommend the\ncensable package.\nFor alternate data unaffected by Census differential privacy, you may want to\nconsider FCC block-level estimates, available using the\nblockpop package.\nTechnical notes\nTo produce election data using 2020 precinct boundaries, election results were\nprojected down to the 2010 block level using voting-age population as weights.\nResults for 2020 blocks were then estimated using 2010 blocks and the\nland-use-based crosswalk files\nfrom VEST. Finally, 2020 blocks were aggregated to 2020 precincts using the\nCensus’ 2020 block assignment files.\n2010 Data Addendum\nIf you are looking for a similar construction for 2010 data, please see here.\n\n\n\n",
    "preview": "https://alarm-redist.github.io/posts/2021-08-10-census-2020/graphic.png",
    "last_modified": "2023-05-23T08:57:47-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-05-revised-das-impact/",
    "title": "Revised: Impact of the Census Disclosure Avoidance System",
    "description": "We are releasing an updated version of our analysis of the U.S. Census'\nprivacy protection system and its impacts on the redistricting process.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\nLast month, the ALARM Project released a report examining “The Impact U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis.” We thank everyone who provided us with feedback on this first version, which was written as a comment on the April 28, 2021 Demonstration Data. The Census Bureau has since updated the system parameters, and as such, we are releasing an updated version.\nRevised report: The Impact of the U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis\nNotably, we expand our study of bias related to racial heterogeneity to include four new states of interest: Alabama, Delaware, Utah, and Washington. We also expand our studies of partisan and racial effects in redistricting to include more cases and provide additional information on the ranges of possible outcomes. These changes come alongside a restructuring of the paper and several smaller edits to recognize additional work focused on the new DAS methodologies used this year by the Census Bureau.With this version, we are also releasing replication data and code, available on the Harvard Dataverse.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-09-dsep-decision-response/",
    "title": "Reaction to the Census Bureau's Updated Parameters",
    "description": "The Data Stewardship Executive Policy Committee announces a higher privacy \nloss budget and other changes to the Disclosure Avoidance System.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-06-09",
    "categories": [],
    "contents": "\nOn June 9, The Census Bureau released information on their final parameters for the 2020 Census data release. We are grateful that they have incorporated some of the recommendations from our report to help build a better data product for redistricting.\nThe Bureau has made several welcome changes. They state that they have updated the post-processing component of the Disclosure Avoidance System (DAS) to address the undercounting bias in racially and ethnically diverse areas, which we reported in our analysis. As we recommended, the Bureau has also increased the privacy-loss budget, allocating the increase towards more accurate population and racial counts on geographies at the block group level and higher. We are hopeful that this targeted increase will help attenuate the DAS- induced population change at the voting district and precinct level.\nSome unresolved issues remain, however. The Bureau indicates that a new demonstration data product will not be released until September. Yet they plan to first release 2020 census redistricting data, which we expect states and localities to begin using immediately, by August 16, 2021.1 Given the timing, it is unclear how the evaluation of the new demonstration data will affect the upcoming redistricting process and related litigation.  The Bureau also states that there will be no direct increases in accuracy for block-level data, in order to protect privacy.  As this is a key aspect of ensuring the equality of vote and protecting the principle of One Person, One Vote, map drawers and analysts may have to adjust their interpretation of this long-standing principle.\nThe Bureau also announced that they plan to release the final version of the DAS code base.  This is an important step for transparency. However, the code alone does not allow analysts to evaluate the impacts of DAS on redistricting and properly account for the additional uncertainty due to the injected noise. We recommend the Bureau also release differentially private noisy population counts that have not been subject to the post-processing steps, as well as parameter values used for noise generation. Although scholarly communities have not fully resolved the issue of incorporating additional noise into redistricting simulation analysis, the availability of such information should facilitate future methodological development. In particular, it is of interest to examine whether or not the additional noise makes it more difficult to detect partisan and racial gerrymandering.\n\nCaliper, who makes the software Maptitude for Redistricting, has announced they will process the legacy format data, and two ALARM Project members have created an R package to process legacy data as well. These types of resources should allow people to immediately begin drawing maps.↩︎\n",
    "preview": "posts/2021-06-09-dsep-decision-response/census_slide.jpg",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-02-das-evaluation-faq/",
    "title": "FAQ: Impact of the Census Disclosure Avoidance System",
    "description": "Answers to common questions about our recently-released report \nevaluating the Census' Disclosure Avoidance System.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-06-02",
    "categories": [],
    "contents": "\n\nContents\nAre you advocating for the use of swapping methods over differential privacy as a method of privacy protection?\nDoes the DAS noise “cancel out” at larger geographic scales?\nWill the noisy data make it harder to create partisan gerrymanders?\nWhy is using a simulation-based method useful to assess DAS? What are the downsides?\nWhy do you use strict thresholds to define equal district population parity, thereby labelling more plans invalid?\nWhy do you use a strict threshold of 50% to define the majority-minority districts (MMDs)?\nWhy does your analysis not adjust for the differential privacy (DP) mechanism, to properly measure uncertainty?\nDo your findings about the prediction of individual race contradict the property of differential privacy?\nWhat are the differences between your analysis and MGGG’s analysis, both of which are based on simulation methods?\nWhy do you rely on Census data as the ‘ground-truth’? Doesn’t the Census itself already have enumeration error?\n\nWe have received a large number of questions and suggestions since releasing the first version of our report on Friday, May 28th. After meeting the Census Bureau’s one month deadline to produce comments on DAS-protected data, we are continuing to undertake additional analyses of the impacts of DAS on the redistricting process and analyses. We are working towards releasing the revised report soon. In the meantime, we provide our current answers to the questions frequently asked by others below.\nAre you advocating for the use of swapping methods over differential privacy as a method of privacy protection?\nNo. We have not studied the impact of swapping on redistricting. It is well known that differential privacy (DP) is theoretically superior to swapping in terms of privacy protection. The key policy question is how much privacy protection we would want at the expense of accurate census measurements. We argue that when changing an important public policy, such as whether additional noise should be added to the census, one needs to carefully determine the impacts of such policy change. We do not argue against DP and believe additional privacy protections to be a worthy consideration for some of the Census variables and products. However, we find that the current implementation of DAS can lead to substantively important changes in redistricting outcomes while also not meaningfully protecting privacy (as demonstrated by the fact that the prediction of individual race remains accurate even with the DAS data). We believe that more studies are needed to determine whether to inject noise and if so how exactly noise should be added. We hope that our study, along with many others, is the first step in answering this important question, by showing the potential consequences of the current DAS on certain redistricting outcomes.\nDoes the DAS noise “cancel out” at larger geographic scales?\nTheoretically yes, but with a caveat. The DAS is designed to have less noise at larger geographic scales, especially for levels of geography in the Census hierarchy, known as “on-spine” geographies (blocks, block groups, tracts, and counties). But geographies like Census places and voting precincts (VTDs) are “off-spine”. The design of the DAS may induce noise that does not cancel out in off-spine geographies.\nOur analysis finds that not only is there more noise for VTDs, but also that there remains a particular form of previously undiscussed bias — perhaps an unintentional side-effect of the DAS post-processing procedure needing to satisfy accuracy constraints in on-spine geographies. We find that the DAS data systematically undercounts racially and politically diverse VTDs in comparison to more homogeneous VTDs. How these discrepancies add up into legislative districts clearly depends on the spatial adjacency of diverse and homogenous VTDs. But in some cases the bias does not cancel out. In Pennsylvania, the average Congressional district changes by only 400 or so people. But the majority-Black 3rd Congressional District gains around 2,000 people under the DAS-protected data, while the more diverse 2nd Congressional District, a Black-Hispanic coalition district, loses around 2,000 people.\nWill the noisy data make it harder to create partisan gerrymanders?\nIn general, no. Partisan gerrymanders can be made with election data and voter files, both of which are not part of the Census and so will not contain any DAS-based error. Other residential and political data sources (both public and private) exist as well, which could be used to draw skewed districts. However, for analysts who must use noisy Census data to identify and analyze a potential gerrymander, the noise and the partisan biases discussed above may make it more difficult to do so.\nWhy is using a simulation-based method useful to assess DAS? What are the downsides?\nWithout the use of simulation, we are restricted to verifying changes in enacted redistricting plans. Analyzing enacted plans under a particular DAS dataset may not allow us to understand how DAS affects other similar plans. By using simulation methods, we are able to draw new maps with the DAS data under realistic constraints while taking into account the geography and spatial distributions of populations.  A downside of simulation is that the results will depend on the types of redistricting maps the algorithms are designed to generate.  More studies are needed to better understand how DAS affects different types of redistricting maps.\nWhy do you use strict thresholds to define equal district population parity, thereby labelling more plans invalid?\nThe use of strict thresholds is informed by a long series of Supreme Court decisions which establish and define the “one person, one vote” principle.  Current practice for redistricting congressional districts is to minimize the population difference across as much as possible.  As summarized by the National Conference of State Legislatures, in many states, this means getting district populations to be within a single person of one another, based on the Census counts.  If only noisy data is available, drawing districts to minimize the population difference becomes at best ambiguous.  Our analysis demonstrates the current statutory and judicial standards may not be applicable to the DAS-protected data.  On this point, our findings about the magnitude of population changes agree with many other analyses of the DAS and the April 28 Demonstration Data.  If the DAS data will be used for redistricting, the key question will be if and how to change the interpretation of “one person, one vote” principle and how such a change will affect redistricting in practice.\nWhy do you use a strict threshold of 50% to define the majority-minority districts (MMDs)?\nWe use a 50% threshold for majority minority districts to remain in line with the jurisprudence following Thornburg v. Gingles (1986). Specifically, Bartlett v. Strickland (2009) held that a minority must “constitute a numerical majority of the voting-age population in an area before §2 requires the creation of a legislative district to prevent dilution of that group’s votes.” When simulating districts, the number of districts which satisfy this specific numerical constraint is thus important. There are other ways to think about minority voting in elections, such as “opportunity districts”, “coalitional districts”, “crossover districts”, and “influence districts.” These have less concrete definitions and rely on electoral data, rather than just Census data.  Certainly, strict majorities are not the only relevant measure of minority voting power, and its coarseness makes it sensitive to change introduced by the DAS.  We find that the practice of using strict thresholds for majority minority districts may lead to biased results especially for small districts such as state legislative districts and school boards.\nWhy does your analysis not adjust for the differential privacy (DP) mechanism, to properly measure uncertainty?\nOur analysis examines the consequences of a map drawer taking DAS-protected data as-is. Ideally, analysts can also adjust for the DP mechanism into that data.  However, the Bureau’s deterministic and asymmetric post-processing procedure cannot be easily incorporated into standard redistricting analyses, preventing analysts from quantifying the added uncertainty exactly.  We recommend that the Bureau release the DP data without post-processing, allowing analysts to adjust redistricting plans with this added uncertainty. \nDo your findings about the prediction of individual race contradict the property of differential privacy?\nTechnically, no. Differential privacy guarantees that the inference based on a database does not depend on whether or not a particular individual is included in the data.  However, one might argue that for the PL94-171 data, the only sensitive information to be protected is race.  Our prediction methodology combines the census block level racial composition with the publicly available addresses and names of registered voters.  We found that our overall prediction performance does not degrade even if we use the noise-added census block data. And, yet, in our reanalysis of a recent court case where this prediction methodology was prominently used, the prediction appears to be substantively different.  We are in the process of studying when and how these differences arise.  In general, when deciding what privacy to protect at the cost of inaccurate measurements, it is important to consider the consequences of privacy disclosure.  Our finding suggests that the addition of noise to the census data does not necessarily improve the privacy protection in terms of individuals’ race.\nWhat are the differences between your analysis and MGGG’s analysis, both of which are based on simulation methods?\nThe MGGG Redistricting Lab has also released a study on using DAS data for redistricting purposes, which we believe is well done. In many ways, our results agree with those of the MGGG team. Like the MGGG study, we also find district-level population errors on the scale of hundreds or thousands of people depending on the size of the district (including Section 5 in our analysis and Figure 4 in the MGGG analysis). The core differences in these results come from interpretation. We find similarly sized errors and consider them to be major differences given that districts in many scenarios are drawn to be as exact as possible, down to individual people in many states. For example, Karcher v. Daggett (1983) found that even minute differences in population parity across congressional districts must be justified, even when smaller than the expected error in the decennial census. The MGGG team interprets their results in light of existing noise in current Census data, and say that “the practice of one-person population deviation across districts was never reasonably justified by the accuracy of Census data nor required by law.”  This implies that if DAS data are to be adopted, courts must decide how to change the interpretation of “one person, one vote” principle.\nOur analysis differs from the MGGG paper in several ways as well, and more studies are needed in order to better understand the impact of DAS on redistricting analysis and evaluation. First, our analysis relies on different data. We use the April 28th Demonstration Data, which the Census says will “closely approximate” the final data product, and span several states. The MGGG team uses the publicly available implementation of the 2018 TopDown algorithm with a reconstruction experiment to study Texas, mostly Dallas County, so our studies do not overlap in geographic areas.\nSecond, our analysis is focused on voting precincts (VTDs), which are the building blocks used in constructing redistricting plans in the vast majority of states.  Election results are also reported at this level or the state equivalent. In contrast, the MGGG analysis built districts out of Census blocks, block groups, and tracts.  These geographies are called “on-spine” by the Census bureau, since they nest within each other. Crucially, the DAS is designed to minimize error for these on-spine geographies, but not for off-spine geographies like VTDs. This may explain why our analysis finds more noise in the VTD population and racial counts than the MGGG’s analysis.\nWhy do you rely on Census data as the ‘ground-truth’? Doesn’t the Census itself already have enumeration error?\nCensus data, as acknowledged by the Bureau, has errors as well, including undercounts of some minority groups. Despite these inaccuracies, we rely on Census data for our simulations because governments and courts generally consider Census data as “ground-truth” for the purposes of drawing districts (e.g., Karcher v. Daggett 1983). The purpose of our analysis is to investigate how relying on DAS data, rather than on Census data as has generally been done in the past, may lead to different results in some cases. In an ideal world, districts drawn using DAS data would be substantively identical to those drawn with Census data. However, we find cases where districts differ by hundreds or thousands of people depending on whether one relies on the Census or DAS data. Additionally, we believe that more research is necessary to determine whether or not existing enumeration errors will cancel out the error induced by DAS, particularly for “off-spine” but substantively important geographies like VTDs.\nAs noted by other researchers, enumeration error is also substantively different from the random error injected as part of the DAS.  At small scales like Census blocks, DAS-injected noise is likely on a larger scale than enumeration error—some Census places have seen their population doubled or halved under the DAS, while such a result is unlikely with enumeration methods. As others have pointed out, this has the effect of adding a large random element on top of the more systematic enumeration errors, which tend to overcount or undercount larger areas or entire minority groups.  DAS does not remove or address these systematic errors; it only adds more error on top of them.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-28-census-das/",
    "title": "Impact of the Census Disclosure Avoidance System on Redistricting",
    "description": "In attempting to protect the privacy of 2020 Census respondents, the Census \nBureau has made its data unsuitable for redistricting purposes.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-05-28",
    "categories": [],
    "contents": "\nThe U.S. Census Bureau plans to protect the privacy of 2020 Census respondents through its Disclosure Avoidance System (DAS), which attempts to achieve differential privacy guarantees by adding noise to the Census data. The Bureau has asked for feedback on the adequacy of DAS-protected data for real-world purposes. The ALARM project has conducted an extensive analysis into how the DAS-protected data affects redistricting and voting rights analyses, and has submitted these findings to the Census Bureau.\nRead the report: The Impact of the U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis\nAlso see: Answers to Frequently Asked Questions Added on June 2, 2021.\nA figure from the report, indicating partisan biases in the DAS-protected data.By applying redistricting simulation and analysis methods to DAS-protected 2010 Census data, we find that the protected data are not of sufficient quality for redistricting purposes. Compared to the original Census 2010 data, we find that the DAS-protected data:\nPrevent map drawers from creating districts that satisfy the One Person, One Vote principle, according to current statutory and judicial standards. Actual deviations from equal population will generally be several times larger than as reported under the DAS data. The magnitude of this problem increases for smaller districts such as state legislative districts and school boards.\nTransfer population from low-turnout, mixed-party areas to high-turnout, single-party areas. This differential bias leads to different district boundaries, which in turn implies significant and unpredictable differences in election results. The discrepancy also degrades the ability of analysts to reliably identify partisan gerrymanders.\nTransfer population from racially mixed areas to racially segregated areas. This bias effectively means racially heterogeneous areas are under-counted. The degree of racial segregation can therefore be over-estimated, which can lead to a change in the number of majority-minority districts. It also creates significant precinct-level variability, which adds substantial unpredictability to whether or not a minority voter is included in a majority-minority district.\nAlter individual-level race predictions constructed from voter names and addresses. This leads to fewer estimated minority voters and majority-minority districts in a re-analysis of a recent Voting Rights Act case, NAACP v. East Ramapo School District. At a statewide level, however, the DAS data does not curb the ability of algorithms to identify the race of voters from names and addresses. Therefore, this casts doubt on the universal privacy protection guarantee of DAS data.\nOur primary recommendation is to release Census P.L. 94-171 data without using the Disclosure Avoidance System, and instead rely on a swapping method similar to that applied to the 2010 Census data in order to protect respondent privacy.\nIf the Census Bureau decides to apply the current DAS to Census PL. 94-171 Data, we recommend increasing the privacy loss budget and allocating the increase to improving redistricting outcomes. In particular, preserving the accuracy of populations at the voting tabulation district level would be critical. The Bureau must avoid injecting noise that systematically undercounts certain racial and partisan groups in the privacy-protected data. Additional recommendations are given in the paper.\n\n\n\n",
    "preview": "posts/2021-05-28-census-das/ex_plot.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-04-02-redist-300/",
    "title": "redist 3.0",
    "description": "A major release brings new algorithms, new workflows, and significant \nusability improvements.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://corymccartan.github.io/"
      },
      {
        "name": "Christopher Kenny",
        "url": "https://www.christophertkenny.com/"
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\nThe ALARM Project is excited to announce the release of redist 3.0 on CRAN. This release brings with it new algorithms and major new workflow improvements, making redistricting analysis broadly accessible to data scientists everywhere.\n\nInstall the new version with install.packages(\"redist\").\nNew Features\nThis release includes far too many changes to list comprehensively. Key improvements and new features include:\nNew tidy interface, including new redist_map and redist_plans objects\nMerge-split MCMC now available in redist_mergesplit()\nShort burst MCMC optimization now available in redist_shortburst() along with scoring functions\nImproved Flip MCMC interface and performance improvements\nNew support for larger simulation size limits\nFunctions to freeze parts of a map and extract district cores\nNew VRA constraint\nMany new plotting functions\nConsistent function and argument names\nNew partisanship and compactnes metrics\nPerformance improvements to compactness calculations\nPlan comparison and classification in compare_plans() and classify_plans()\nNew iowa dataset and cleaned-up package data\nTo begin exploring the new features, check out the new Get Started vignette.\nWorkflow Example: North Carolina\nTo demonstrate the new redist workflow, we’ll run through a basic analysis of the 2017 congressional districts of the state of North Carolina, which were struck down as an unconstitutional partisan gerrymander in 2019.\nNew workflow\n\n\nlibrary(tidyverse)\nlibrary(redist)\n\ndownload.file(\"https://github.com/alarm-redist/redist-data/raw/main/data/nc.rds\",\n              data_path <- tempfile())\nnc_shp <- readRDS(data_path) %>%\n    select(vtd:vap, el14g_uss_r:geometry)\n\n\n\nUnder the new workflow, a redistricting analysis begins with a redist_map object, which defines the basic parameters of the redistricting problem. The redist_map() constructor builds the precinct adjacency graph which is required for redistricting simulation, and stores relevant metadata, such as the desired population parity tolerance and a reference to the existing districts. It also comes with helpful plotting functions.\n\n\nnc = redist_map(nc_shp, existing_plan=cd_17, pop_tol=0.01)\nprint(nc)\n\n\nA redist_map object with 2692 units and 15 fields\nTo be partitioned into 13 districts with population between 733,499 - 1.0% and 733,499 + 1.0%\nWith geometry:\n    bbox:           xmin: 406820 ymin: 2696.2 xmax: 3070200 ymax: 1043600\n    projected CRS:  NAD83(HARN) / North Carolina (ftUS)\n# A tibble: 2,692 x 15\n   vtd       county   pop   vap el14g_uss_r el14g_uss_d el14g_uss_l\n * <chr>     <chr>  <int> <int>       <int>       <int>       <int>\n 1 3700106W  37001   1973  1505         181         182          17\n 2 3700112E  37001   3391  2503         180         271          21\n 3 3700112W  37001   2744  2156         457         481          42\n 4 3700106N  37001   4468  3167         231         466          31\n 5 37001126  37001   2038  1713         670         416          38\n 6 37001124  37001   2455  1948         491         391          33\n 7 370011210 37001   2802  2127         358         309          31\n 8 3700103N  37001   5712  4955        1063         853          53\n 9 3700102   37001   4491  3483        1246         313          62\n10 3700106E  37001   3113  2371         423         432          42\n# … with 2,682 more rows, and 8 more variables: el14g_uss_wi <int>,\n#   el14g_uss_tot <int>, cd_13 <int>, cd_17 <int>, aland10 <dbl>,\n#   awater10 <dbl>, geometry <MULTIPOLYGON [US_survey_foot]>,\n#   adj <list>\n\nplot(nc, el14g_uss_d/(el14g_uss_d+el14g_uss_r)) +\n    scale_fill_gradient2(midpoint=0.5)\n\n\n\n\nOnce we’ve created a redist_map object, we can simulate redistricting plans.\n\n\nplans = redist_smc(nc, 1000, counties=county, silent=TRUE) # 1000 plans\nprint(plans)\n\n\n1000 sampled plans and 1 reference plan with 13 districts from a 2692-unit map,\n  drawn using Sequential Monte Carlo\nWith plans resampled from weights\nPlans matrix: num [1:2692, 1:1001] 6 6 6 6 6 6 6 6 6 6 ...\n# A tibble: 13,013 x 3\n   draw  district total_pop\n   <fct>    <int>     <dbl>\n 1 cd_17        1    733323\n 2 cd_17        2    734740\n 3 cd_17        3    732627\n 4 cd_17        4    733218\n 5 cd_17        5    733879\n 6 cd_17        6    733554\n 7 cd_17        7    734750\n 8 cd_17        8    734777\n 9 cd_17        9    731507\n10 cd_17       10    736057\n# … with 13,003 more rows\n\nThe plans variable is a redist_plans object—a special container designed to make handling sets of redistricting plans painless. As the output above shows, plans contains the 1,000 samppled plans, plus the 2017 congressional districts. We can plot a few of these plans.\n\n\nredist.plot.plans(plans, draws=c(\"cd_17\", \"1\", \"2\", \"3\"), geom=nc)\n\n\n\n\nA redist_plans object makes it easy to compute plan and district summary statistics.\n\n\nplans = plans %>%\n    mutate(comp = distr_compactness(nc),\n           dem_share = group_frac(nc, el14g_uss_d, el14g_uss_d + el14g_uss_r))\nprint(plans)\n\n\n1000 sampled plans and 1 reference plan with 13 districts from a 2692-unit map,\n  drawn using Sequential Monte Carlo\nWith plans resampled from weights\nPlans matrix: num [1:2692, 1:1001] 6 6 6 6 6 6 6 6 6 6 ...\n# A tibble: 13,013 x 5\n   draw  district total_pop  comp dem_share\n   <fct>    <int>     <dbl> <dbl>     <dbl>\n 1 cd_17        1    733323 0.803     0.687\n 2 cd_17        2    734740 0.803     0.443\n 3 cd_17        3    732627 0.803     0.407\n 4 cd_17        4    733218 0.803     0.674\n 5 cd_17        5    733879 0.803     0.425\n 6 cd_17        6    733554 0.803     0.441\n 7 cd_17        7    734750 0.803     0.446\n 8 cd_17        8    734777 0.803     0.439\n 9 cd_17        9    731507 0.803     0.440\n10 cd_17       10    736057 0.803     0.419\n# … with 13,003 more rows\n\nFrom there, we can quickly generate informative plots. First we check the compactness of the generated plans, and see that they are significantly more compact than the adopted 2017 plan.\n\n\nhist(plans, comp) +\n    labs(x=\"Compactness score (higher is more compact)\")\n\n\n\n\nNext, we look at the partisan implications of the 2017 plan. We plot the two-party Democratic vote share in each district, with districts sorted by this quantity. Each dot on the plot below is a district from one simulated plan, and the red lines show the values for the 2017 plan.\n\n\nredist.plot.distr_qtys(plans, dem_share, size=0.1)\n\n\n\n\nWe see immediately that the 2017 plan packs Democratic voters into the three most Democratic districts, and cracks them in the remaining 10 districts, leading to a durable 10–3 Republican-Democratic seat split (in an election which Democrats captured 49% of the statewide two-party vote). A clear partisan gerrymander.\nStudying districts 1, 2, and 4\nIf we want to study a specific set of districts, we can quickly filter() to the relevant map area and re-run the analysis. The redist_map() object will handle all appropriate adjustments to the adjacency graph, number of districts, and population tolerance (as is visible below).\n\n\nnc_sub = filter(nc, cd_17 %in% c(1, 2, 4))\nprint(nc_sub)\n\n\nA redist_map object with 571 units and 15 fields\nTo be partitioned into 3 districts with population between 733,760 - 1.0353% and 733,760 + 0.96399%\nWith geometry:\n    bbox:           xmin: 1921600 ymin: 524880 xmax: 2784100 ymax: 1028200\n    projected CRS:  NAD83(HARN) / North Carolina (ftUS)\n# A tibble: 571 x 15\n   vtd     county   pop   vap el14g_uss_r el14g_uss_d el14g_uss_l\n * <chr>   <chr>  <int> <int>       <int>       <int>       <int>\n 1 37015C2 37015   2182  1707         174         503          13\n 2 37015M1 37015   1103   849         172         167           5\n 3 37015C1 37015   1229   986         229         184          11\n 4 37015MH 37015    992   811         146         254          12\n 5 37015W2 37015    966   764         286          47          20\n 6 37015W1 37015   7005  5703         596        1190          44\n 7 37015M2 37015   1290   983          99         239          16\n 8 37015SN 37015   1410  1025          63         327          11\n 9 37015WH 37015   1554  1274         292         262          12\n10 37015WD 37015   1409  1050          35         319           5\n# … with 561 more rows, and 8 more variables: el14g_uss_wi <int>,\n#   el14g_uss_tot <int>, cd_13 <int>, cd_17 <int>, aland10 <dbl>,\n#   awater10 <dbl>, geometry <MULTIPOLYGON [US_survey_foot]>,\n#   adj <list>\n\nplot(nc_sub)\n\n\n\n\nOn this subset, too, the adopted 2017 plan is a significant outlier.\n\n\nplans_sub = redist_smc(nc_sub, 1000, counties=county, silent=T) %>%\n    mutate(dem_share = group_frac(nc_sub, el14g_uss_d, el14g_uss_d + el14g_uss_r))\nredist.plot.distr_qtys(plans_sub, dem_share, size=0.3)\n\n\n\n\nOld workflow\nIn comparison, the old workflow required significantly more steps and manual processing.\n\n\nlibrary(tidyverse)\nlibrary(redist)\n\ndownload.file(\"https://github.com/alarm-redist/redist-data/raw/main/data/nc.rds\",\n              data_path <- tempfile())\nnc_shp <- readRDS(data_path) %>%\n    select(vtd:vap, el14g_uss_r:geometry)\n\n\n\nOnce we’ve downloaded the data, we can start by building the adjacency graph.\n\n\nadj <- redist.adjacency(nc_shp)\n\n\n\nTime to first simulation was never really the issue, however each simulation required many inputs. redist_map objects keep track of the adj, total_pop, ndists, and pop_tol arguments, but in the older version, you had to specify each of these for every simulation. One of the quirky aspects of the older version was that counties needed to be a vector with values 1:n_counties, meaning that you had to manually transform it to use it and that only worked if the counties were contiguous.\n\n\nsims <- redist.smc(adj = adj, total_pop = nc_shp$pop, ndists = 13, \n                   pop_tol = 0.01, \n                   counties = match(nc_shp$county, unique(nc_shp$county)), \n                   nsims = 1000, silent = TRUE)\n\n\n\nOnce you finished simulating, setting up plots was always a hassle, as you needed to plot both the distribution of simulations and then compute the same metric separately for the reference plan, in this case that’s the 2017 congressional districts.\n\n\nmetrics <- redist.metrics(plans = sims$plans, measure = 'DVS',\n                          rvote = nc_shp$el14g_uss_r, nc_shp$el14g_uss_d)\n\nsorted <- metrics %>% \n  group_by(draw) %>% \n  arrange(DVS,.by_group = TRUE) %>% \n  mutate(district = 1:13) %>% \n  ungroup()\n\nreference_metrics <- redist.metrics(plans = nc_shp$cd_17, \n                                    measure = 'DVS', \n                                    rvote = nc_shp$el14g_uss_r, \n                                    dvote = nc_shp$el14g_uss_d)\n\nsorted_reference <- reference_metrics %>% \n    arrange(DVS) %>% \n    mutate(district = 1:13)\n\n\n\nAnd then to plot the standard stacked boxplots, you would need to add the reference plan manually to the rest.\n\n\nsorted %>% ggplot(aes(x = district, y = DVS, group = district)) + \n  geom_boxplot() + \n  theme_minimal() + \n  labs(x = 'District, sorted by DVS') + \n  geom_segment(data = sorted_reference, size = 1,\n               aes(x = district - 0.35, xend = district + 0.35, \n                   yend = DVS, color = 'red')) + \n  scale_color_manual(name = '', values = c('red' = 'red'),\n  labels = c('ref'), guide = 'legend')\n\n\n\n\nStudying districts 1, 2, and 4\nThe steps between loading in data to your first simulation wasn’t terrible in the old version when you were working with the full map. However, when trying to work with subsets, it became messy.\nFirst you needed to subset the shape and then rebuild a new adjacency graph that only had the remaining precincts.\n\n\nsub <- nc_shp %>% filter(cd_17 %in% c(1, 2, 4))\nsub_adj <- redist.adjacency(sub)\n\n\n\nThen, if your target on the full map was 1%, you had to compute the equivalent on the subset map, as a 1% population deviation on a subset is often larger once recombined with the full map.\n\n\npop_tol <- 0.01\nsubparpop <- sum(sub$pop)/3\nparpop <- sum(nc_shp$pop)/13\n\nsub_pop_tol <-  min(abs(subparpop - parpop * (1 - pop_tol)),\n                abs(subparpop - parpop * (1 + pop_tol))) / subparpop\nsub_pop_tol\n\n\n[1] 0.0096399\n\nNow we can simulate again, but on the smaller map.\n\n\nsims_sub <- redist.smc(adj = sub_adj, total_pop = sub$pop,\n                      nsims = 1000,  ndists = 3, \n                      counties = match(sub$county, unique(sub$county)),\n                      pop_tol = sub_pop_tol, silent = TRUE)\n\n\n\nAs before, we have to compute metrics for both the reference plan and the simulated plans.\n\n\nsub_metrics <- redist.metrics(plans = sims_sub$plans, measure = 'DVS', \n                              rvote = sub$el14g_uss_r, sub$el14g_uss_d)\n\nsub_sorted <- sub_metrics %>% \n  group_by(draw) %>% \n  arrange(DVS,.by_group = TRUE) %>% \n  mutate(district = 1:3) %>% \n  ungroup()\n\nsub_reference_metrics <- redist.metrics(plans = match(sub$cd_17, \n                                                      unique(sub$cd_17)), \n                                        measure = 'DVS', \n                                        rvote = sub$el14g_uss_r, \n                                        dvote = sub$el14g_uss_d)\n\nsub_sorted_reference <- sub_reference_metrics %>%\n    arrange(DVS) %>% \n    mutate(district = 1:3)\n\n\n\nAnd finally, we can plot the metrics and manually add the reference points.\n\n\nsub_sorted %>% ggplot(aes(x = district, y = DVS, group = district)) + \n  geom_boxplot() + \n  theme_minimal() + \n  labs(x = 'District, sorted by DVS') + \n  geom_segment(data = sub_sorted_reference, size = 1,\n               aes(x = district - 0.35, xend = district + 0.35, \n                   yend = DVS, color = 'red')) + \n  scale_color_manual(name = '', values = c('red' = 'red'),\n  labels = c('ref'), guide = 'legend')\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-02-redist-300/redist-300_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
